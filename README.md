# Awesome-Multimodal-Video [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)



## Table of Contents

* [Main](#main)
  * [Video Language Transformers](#video-language-transformers)
  * [Video Retrieval](#video-retrieval)
  * [Video Question Answering](#video-question-answering)
  * [Video Captioning](#video-captioning)
* [Datasets and SOTA](#datasets-and-sota)
   * [Large-scale Video Language Dataset](#large-scale-video-language-dataset)
   * [Downstream Tasks](#downstream-tasks)




# Main

## Video Language Transformers

  * **VIOLETv2 (EmpiricalMVM)**: "An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling", CVPR 2023.
    <br/>[[Paper](https://arxiv.org/abs/2209.01540)][[Code](https://github.com/tsujuifu/pytorch_empirical-mvm)] #Microsoft

  * **LAVENDER**: "LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling", CVPR 2023.
    <br/>[[Paper](https://arxiv.org/abs/2206.07160)][[Code](https://github.com/microsoft/LAVENDER)] #Microsoft

  * **Flamingo**: "Flamingo: a Visual Language Model for Few-Shot Learning", NeurIPS 2022.
    <br/>[[Paper](https://arxiv.org/abs/2204.14198)] #DeepMind

  * **MERLOT Reserve**: "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound"

  * **VL-Adapter**: "VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"

  * **MERLOT**: "MERLOT: Multimodal Neural Script Knowledge Models"

  * **VIOLET**: "VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling", arXiv 2021.
    <br/>[[Paper](https://arxiv.org/abs/2111.12681)][[Code](https://github.com/tsujuifu/pytorch_violet)] #Microsoft

  * **HERO**: "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"

  * **UniVL**: "UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"

## Video Retrieval

  * **FiT**: "Frozen in Time: ️A Joint Video and Image Encoder for End to End Retrieval", ICCV 2021.
    <br/>[[Paper](https://arxiv.org/abs/2104.00650)][[Code](https://github.com/m-bain/frozen-in-time)][[Website](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time)][[Demo](http://meru.robots.ox.ac.uk/frozen-in-time)] #Oxford

## Video Question Answering

  * **FrozenBiLM**: "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models"

  * **JustAsk**: "Just Ask: Learning to Answer Questions from Millions of Narrated Videos"
    <br/>[[Paper](https://arxiv.org/abs/2012.00451)][[Code](https://github.com/antoyang/just-ask)][[Website](https://antoyang.github.io/just-ask.html)][[Demo](http://videoqa.paris.inria.fr/)][[Poster](https://antoyang.github.io/slides/just-ask-iccv-poster.pdf)][[Slides](https://antoyang.github.io/slides/just-ask-iccv.pdf)][[Oral](https://youtu.be/jzXdRT5W3C4?t=17280)] #Inria

## Video Captioning

  * **Video ChatCaptioner**: "Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions"

  * **Vid2Seq**: "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning", CVPR 2023.
    <br/>[[Paper](https://arxiv.org/abs/2302.14115)][[Code](https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq)][[Website](https://antoyang.github.io/vid2seq.html)][[Blog](https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html)] #Google

  * **SwinBERT**: "SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning"

  * "Zero-Shot Video Captioning with Evolving Pseudo-Tokens"

# Datasets and SOTA

## Large-scale Video Language Dataset

  * **WebVid-10M**: "Frozen in Time: ️A Joint Video and Image Encoder for End to End Retrieval", ICCV 2021.
  <br/>[[Paper](https://arxiv.org/abs/2104.00650)][[Code](https://github.com/m-bain/webvid)][[Website](https://m-bain.github.io/webvid-dataset)] #Oxford

  * **HowTo100M**: "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips", ICCV 2019.
  <br/>[[Paper](https://arxiv.org/abs/1906.03327)][[Code](https://github.com/antoine77340/howto100m)][[Website](https://www.di.ens.fr/willow/research/howto100m)] #Inria

## Downstream Tasks

  * **QVHighlights**: "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries"

  * **VidSitu**: "Visual Semantic Role Labeling for Video Understanding"

  * **How2QA**: "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"

  * **How2R**: "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
 
  * **TVC**: "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval"

  * **TVR**: "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval"

  * **TVQA**: "TVQA: Localized, Compositional Video Question Answering"

  * **MSR-VTT**: "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"

  * **YouCook2**: "Towards Automatic Learning of Procedures from Web Instructional Videos"

  * **Charades**: "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding"

  * **DiDeMo**: "Localizing Moments in Video with Natural Language"