# Awesome-Multimodal-Video

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)


## Table of Contents

* [Main](#main)
  * [Video Language Transformers](#video-language-transformers)
  * [Video Retrieval](#video-retrieval)
  * [Video Localization](#video-localization)
  * [Video Question Answering](#video-question-answering)
  * [Video Captioning](#video-captioning)
  * [Video Dense Captioning](#video-dense-captioning)
* [Datasets and SOTA](#datasets-and-sota)




# Main

## Video Language Transformers

  * **Flamingo**: "Flamingo: a Visual Language Model for Few-Shot Learning", NeurIPS 2022.
  <br/>[[Paper](https://arxiv.org/abs/2204.14198)] #DeepMind

## Video Retrieval

## Video Localization

## Video Question Answering

## Video Captioning

## Video Dense Captioning

  * **Vid2Seq**: "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning", CVPR 2023.
  <br/>[[Paper](https://arxiv.org/abs/2302.14115)][[Code](https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq)][[Website](https://antoyang.github.io/vid2seq.html)][[Blog](https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html)] #Google

# Datasets and SOTA

